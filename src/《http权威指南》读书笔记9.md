## 概述

最近对http很感兴趣，于是开始看[《http权威指南》](https://book.douban.com/subject/10746113/)。别人都说这本书有点老了，而且内容太多。我个人觉得这本书**写的太好了**，非常长知识，让你知道关于http的很多概念，不仅告诉你怎么做，还告诉你为什么这么做。于是我把学到的知识点记录下来，供以后开发时参考，相信对其他人也有用。

### web机器人

1.web机器人也叫做**自活跃用户代理**，它能够在无需人类干预的情况下自动进行一系列web事务处理的软件程序。

2.**web爬虫或蜘蛛**是一种机器人，它们会递归地对各种信息性web站点进行遍历，获取第一个 web页面，然后获取那个页面指向的所有web页面，然后是那些页面指向的所有web页面，以此类推。

3.爬虫开始访问的URL初始集合被称为**根集**，根集不需要很多页面，就可以涵盖一大片web结构，通常一个好的根集会包括一些大的流行的web站点。

4.机器人在web上爬行时，要特别小心不要陷入循环，至少出于下列按个原因，**环路**对爬虫来说是有害的：
- 爬虫会不停的兜圈子，消耗掉很多网络带宽。
- 另一端的web服务器也会遭受打击，甚至有可能会击垮web站点从而被诉讼。
- 爬虫应用程序会被重复内容充斥，这样应用程序会变得毫无用处。

5.**大规模web爬虫**使用的一些有用技术：
- 树和散列表
- 有损的存在位图
- 检查点
- 分类

6.机器人先通过下列步骤将每个URL转化为**规范化**的格式：
- 如果没有指定端口的话，就向主机名中添加“：80”。
- 将所有转义符%xx都转换成等价字符。
- 删除#标签。

7.为避免机器人会遇到**各种危险的web**，可以使用下面的技术：
- 规范化URL
- 广度优先的爬行
- 节流
- 限制URL的大小
- 站点黑名单
- 模式检测（md5校验和）
- 人工监视

8.服务器可能会为它所处理的内容提供一些首部，但是**标签http-equiv**为内容编写者提供了一种覆盖这些首部的方式：

```
<meta http-equiv="Refresh" content="1;URL=index.html">
```

9.所有web服务器都可以在服务器的文档根目录中提供一个可选的、名为**robots.txt**的文件，这个文件包含的信息说明了机器人可以访问服务器的哪些部分。

10.比如可以通过访问https://www.bilibili.com/robots.txt获取**B站的robots文件**，内容如下：

```
User-agent: *
Disallow: /include/
Disallow: /mylist/
Disallow: /member/
Disallow: /images/
Disallow: /ass/
Disallow: /getapi
Disallow: /search
Disallow: /account
Disallow: /badlist.html
Disallow: /m/
```









